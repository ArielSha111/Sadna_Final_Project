{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ffc09e",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765d80e",
   "metadata": {},
   "source": [
    "***In this notebook we present the Data gathering process for this project***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeec8b3",
   "metadata": {},
   "source": [
    "This process is done the following steps: \n",
    "\n",
    "1) Downloading and converting (to .txt* format) the companies' 10-Ks. \n",
    "\n",
    "   Tools: web scraping, using a RPA Bot. \n",
    "\n",
    "2) Extracting section \"1.A Risk Factors\" . \n",
    "\n",
    "\n",
    "In the follwoing section we will discuss our progress, regarding the Data gathering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb2d7b",
   "metadata": {},
   "source": [
    "- - -  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a3173",
   "metadata": {},
   "source": [
    "## 1. Downloading and converting the 10-K reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9fca6",
   "metadata": {},
   "source": [
    "In order to dowload tna convert the 10-K reports, we used RPA Bot.\n",
    "\n",
    "Robotic Process Automation (RPA) is a software technology, used to automate digital tasks. RPA Bots can interact with any system or application the same way a human worker would, by watching the user perform tasks in the application's graphical user interface (GUI). Thereafter, perform the automation by repeating those tasks directly in the GUI. \n",
    "For this project, the RPA Bot was created with the platform of UiPath.\n",
    "\n",
    "The Bot was designed to:\n",
    "\n",
    "    •\tExtracting a company's CIK and year of Cyber Incidents \n",
    "    •\tOpen EDGAR data base \n",
    "    •\tType the designated CIK    \n",
    "    •\tLocate the relevant 10-K report (if exists)  \n",
    "    •\tSaving the report locally    \n",
    "    •\tIterate \n",
    "    \n",
    "Initially, there were circa 2,000 cases of Cyber Incidents. In some cases, a company has experienced more than one incident in a given year. Therefore, the Bot collected a smaller number of reports in comparison with the date base size. Moreover, for some companies there was no 10-K report available from different reasons, such as mergers and acquisitions, several CIKs per company (e.g. Jpmorgan Chase & Co.). \n",
    "\n",
    "Eventually, untill the day of submission, we managed to retrieve approximately 700 report from each group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f38e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec5c52",
   "metadata": {},
   "source": [
    "## (*)  Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd3cf4",
   "metadata": {},
   "source": [
    "We started this essential part of all the important libraries and downloads we needed in this section of the work. Those directories were carefully selected in light of their suitability for the nature of our data processing and even its initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09b2d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import shutil\n",
    "import string\n",
    "from string import punctuation\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec18b33",
   "metadata": {},
   "source": [
    "## (*) Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4483274",
   "metadata": {},
   "source": [
    "Now we actually defined their variables we needed in a large part of the notebook, those variables that retain key values like names of important directories, lists of files, words that indicate important sections in each file we will work on and more. The definition of these variables at the beginning of this section was very important as this way we could make minimal changes to the code should we need to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a43a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_with_data = 'Data'  # that's the directory name of the directory that contains all of the raw data\n",
    "new_dir_with_outcome = 'Risk' + dir_with_data # that's the directory name of the directory that will contain all of the outcomes\n",
    "file_names = []  # that's the list of the names of all the files from the Data directory\n",
    "start_key_words = ['ariel','1A.', 'Risk Factors\\n',\n",
    "                 'RISK FACTORS\\n']  # that's the list of the keywords that indicates if we are on the needed section\n",
    "start_key_word_counters = []  # that's the list of counters to count how many times each of the kew words appeared\n",
    "end_key_words = ['ITEM 1B.', 'Item 1B.', 'ITEM 1B',\n",
    "               'Item 1B', 'UNRESOLVED STAFF COMMENTS', 'Unresolved Staff Comments',\n",
    "               'Properties']  # that's the list of key words that indicates if the section is over\n",
    "created_files_counter = 1\n",
    "words_to_clean = []\n",
    "project_directories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb05da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc300a18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fa472",
   "metadata": {},
   "source": [
    "## 2. Extracting section \"1.A Risk Factors\"\n",
    "\n",
    "In this part we validate and manipulate the text in the files, in order to extract section \"1A. Risk Factors\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ffba0",
   "metadata": {},
   "source": [
    "We now went through virtually every file in the Data folder and cut out only the parts we wanted to work on. Those parts are characterized by a title that in most cases is called Item 1A Risk factors and therefore for the same cut from each file we are called Risk Section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48a9d8",
   "metadata": {},
   "source": [
    "***Check: if a word  exists in a line***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9603d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_in_line(word, line):\n",
    "    if line.find(word) != -1:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaba735",
   "metadata": {},
   "source": [
    "***Check: if the string names line contains any of the starts key words***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9dc8a6",
   "metadata": {},
   "source": [
    "Here we literally tested whether a current line is actually the title of that Risk Section mentiond above.Finding the critical break is problematic. The files are in a slightly different format and it is important to note that from a large set of data we were able to find the kitty part in a large percentage but certainly not at all the files. The ambition was to reach as large a file as possible but it was quite difficult and we are definitely happy with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db78c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_risk_paragraph_line(line):\n",
    "    index = 0\n",
    "    for word in start_key_words:\n",
    "        if is_word_in_line(word, line):\n",
    "            if start_key_word_counters[index] == 0:\n",
    "                start_key_word_counters[index] = 1\n",
    "                break\n",
    "            elif start_key_word_counters[index] == 1:\n",
    "                return True\n",
    "        index += 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62d673",
   "metadata": {},
   "source": [
    "***Check: if a line contains any key words that indicate reaching the end of a the section***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe657a78",
   "metadata": {},
   "source": [
    "And Here we tested whether a current line is actually the end of that Risk Section. Similar to finding the beginning of the critical part, finding the end was also problematic. It is important to note that it was slightly easier to find a formal definition for finding the end of the required part. But of course there is little room for improvement and optimality, as I mentioned above we are very pleased with the results so this is the current situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5c5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_risk_paragraph_ended(line):\n",
    "    global created_files_counter\n",
    "    for word in end_key_words:\n",
    "        if is_word_in_line(word, line):\n",
    "            created_files_counter += 1\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc427f",
   "metadata": {},
   "source": [
    "We started the process having 672 text files, which the RPA Bot downloaded. After extracting section 1A. Risk Factors, we ended up with 385 usable files, after cleaning empty files and files in which we extracted different sections. We will try to improve this part of the data gathering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24d2c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c89ee",
   "metadata": {},
   "source": [
    "### 2.1. Cleaning exceptional formatting characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23274c36",
   "metadata": {},
   "source": [
    "We have now created functions for minimal clearing of the information by deleting problematic characters and things left over as part of the information mining process. It is important to note that this part is not the original cleansing of the data but only a very basic processing of problematic characters and certain cues that tend to be found on websites. Without this part we actually encountered many problems later on which is why the cleaning operation is so critical at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac68d5a",
   "metadata": {},
   "source": [
    "***Check: if a char is valid in this format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78be1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_char(c):\n",
    "    if c.isdigit() or c.isalpha() or c =='\\n' or c =='' or c==' ': \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920debc",
   "metadata": {},
   "source": [
    "***Get a valid chars line (digits, alphabet and white)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d726a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_line(line):  \n",
    "    first_char = line[0]  \n",
    "    if len(line) > 1 and not first_char.isdigit() and not first_char.isalpha():\n",
    "        line = line[1:]   \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb05da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94677d03",
   "metadata": {},
   "source": [
    "### 2.2. Creating \"Risk Files\" and saving them locally\n",
    "\n",
    "\"Risk Files\" are the new files, containing the valideated text of section \"1A. Risk Factors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ea251",
   "metadata": {},
   "source": [
    "And now, having created functions for trimming all the required information we have created functions that cut each file individually and save the result locally. This is by opening a new file after each trimming is done. That means we actually create new files with the same name of the original files only with the addition of the word Risk. The same files contain the critical part for processing. All this is done so that we can distinguish between the files before and after the change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dadd7d1",
   "metadata": {},
   "source": [
    "***Make a buffer of words from risk section***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea5c20",
   "metadata": {},
   "source": [
    "In the next bluck we have created a function that lists the Risk part within a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8738da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_buffer(file_buffer_to_read):\n",
    "    cur_buffer_to_write = [] \n",
    "    in_risk_paragraph = False\n",
    "    for line in file_buffer_to_read:\n",
    "        if not in_risk_paragraph and in_risk_paragraph_line(line):\n",
    "            in_risk_paragraph = True                    \n",
    "        elif in_risk_paragraph and is_risk_paragraph_ended(line):\n",
    "            break\n",
    "        if in_risk_paragraph:           \n",
    "            clean_line = get_clean_line(line)\n",
    "            cur_buffer_to_write.append(clean_line)\n",
    "    return cur_buffer_to_write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764e57a",
   "metadata": {},
   "source": [
    "***Write a buffer to a new file : a given file name + 'Risk'***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d77fc",
   "metadata": {},
   "source": [
    "We have now created a function that writes the same part of Risk stored locally in a file with the same name plus Risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983ef456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_buffer_to_file(cur_buffer_to_write, file_name, new_dir_with_outcome):\n",
    "    file_to_write = open('results\\\\' + new_dir_with_outcome + '\\\\' + 'Risk' + file_name, 'w', encoding='utf-8')\n",
    "    try:\n",
    "        for line in cur_buffer_to_write:\n",
    "            file_to_write.write(line + \"\\n\")\n",
    "    finally:\n",
    "        file_to_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeda33a",
   "metadata": {},
   "source": [
    "***Create a risk file and assigning it to the a directory***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608570d",
   "metadata": {},
   "source": [
    "Here we have defined the function that will do the important actions generically for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15efde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_risk_file(file_buffer_to_read, file_name):\n",
    "    global start_key_word_counters \n",
    "    start_key_word_counters = [0] * (len(start_key_words))\n",
    "    cur_buffer_to_write = get_risk_buffer(file_buffer_to_read)\n",
    "    write_list_buffer_to_file(cur_buffer_to_write, file_name, new_dir_with_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227f3bd",
   "metadata": {},
   "source": [
    "***Create all risk files for Data files***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb210b",
   "metadata": {},
   "source": [
    "And here we have done all the cutting and recreating of the files for on the files in the Data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baede530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_risk_files():\n",
    "    for file_name in file_names:\n",
    "        with open(dir_with_data  + '\\\\' + file_name, 'r', encoding='utf-8') as f:\n",
    "            file_buffer_to_read = f.readlines()\n",
    "        create_risk_file(file_buffer_to_read, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561380a",
   "metadata": {},
   "source": [
    "***Create a directory with a given name***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b3e16",
   "metadata": {},
   "source": [
    "Now in the last step before calling the functions and creating the new files, we had to save the new files in a dedicated folder so that they are easy to use. So basically we created another function that creates for us a new directory with a significant name that matches the folder name of the original data. The goal in creating all of these folders is easy and quick access to the various files during the process. But perhaps more important than that the goal is to keep the original data intact in such a way that we will not have to change it and woe to it. You can always delete what we created and actually start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9dd42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    path = os.path.join(dir_name)\n",
    "    os.mkdir(path)\n",
    "    project_directories.append(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8272f85d",
   "metadata": {},
   "source": [
    "#### creating all the risk files in a needed directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7251de",
   "metadata": {},
   "source": [
    "And finally after we have defined all the required functions we can call them now in order to create the new files for each section of Risk within a new folder designed for that. And so we certainly did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87251f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir('results')\n",
    "file_names = os.listdir(dir_with_data)\n",
    "create_dir('results\\\\' + new_dir_with_outcome)\n",
    "create_all_risk_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9369f",
   "metadata": {},
   "source": [
    "And now we are in the situation that in the notebook folder we got a new folder that contains all the parts that interest us for data analysis, these data are not clean and contain unnecessary characters and information but are definitely the critical parts and information which is an essential part of data analysis for this project. In the next steps we will see how we continued the data processing process before their analysis and how we dealt with various problems in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb05da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa2b40",
   "metadata": {},
   "source": [
    "### 2.3. Creating a corpus from the Risk Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d5263",
   "metadata": {},
   "source": [
    "Now that we have the important sections we have created a corpus from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbea303",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = 'results\\\\' + new_dir_with_outcome \n",
    "risk_files_corpus = PlaintextCorpusReader(corpus_root, '.*') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb05da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c03d74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af41d9",
   "metadata": {},
   "source": [
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72e2d5",
   "metadata": {},
   "source": [
    "In Part 1. Data Gathering we reviewed the three data bases we use for this project. Also, we discribed the data collection process, in which managed to ***(1)*** download the 10-K files, using a RPA Bot, ***(2)*** extract the specific section \"1A. Risk Factors\". \n",
    "\n",
    "Finally, we manage to create around 700 workable file in total (both groups together) , containing the \"item 1A. Risk Factor\". Please see 1.4 Description- control group for further information. \n",
    "\n",
    "Please note:  we will refer the report files from now on as the observations of the data set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
